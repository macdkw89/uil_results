{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define column headers for csv export\n",
    "columns_21 = ['Event', 'School', 'TEA', 'City', 'Directors', 'Conference', 'Classification', 'Year', 'ID', \n",
    "    'Stage Judge 1', 'Stage Judge 2', 'Stage Judge 3', 'Stage Final', \n",
    "    'SR Judge 1', 'SR Judge 2', 'SR Judge 3', 'SR Final', 'Award', \n",
    "    'Selection 1', 'Selection 2', 'Selection 3', 'Date', 'Region', 'cj1', 'cj2', 'cj3', 'srj1', 'srj2', 'srj3']\n",
    "columns_22 = ['Event', 'School', 'TEA', 'City', 'Directors', 'Conference', 'Classification', 'Year', 'ID', \n",
    "    'Stage Judge 1', 'Stage Judge 2', 'Stage Judge 3', 'Stage Final', \n",
    "    'SR Judge 1', 'SR Judge 2', 'SR Judge 3', 'SR Final', 'Award', \n",
    "    'Selection 1', 'Selection 2', 'Selection 3', 'Date', 'Region', 'cj1', 'cj2', 'cj3', 'srj1', 'srj2', 'srj3', 'oops']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group the CSVs together, and fix rows with Accompanist error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of merged files returned\n",
    "files = glob.glob(\"full_run/*.csv\")\n",
    "\n",
    "# joining files with concat and read_csv\n",
    "df = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# drop unnamed column\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# name the columns\n",
    "df.columns = columns_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event</th>\n",
       "      <th>School</th>\n",
       "      <th>TEA</th>\n",
       "      <th>City</th>\n",
       "      <th>Directors</th>\n",
       "      <th>Conference</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Year</th>\n",
       "      <th>ID</th>\n",
       "      <th>Stage Judge 1</th>\n",
       "      <th>...</th>\n",
       "      <th>Selection 2</th>\n",
       "      <th>Selection 3</th>\n",
       "      <th>Date</th>\n",
       "      <th>Region</th>\n",
       "      <th>cj1</th>\n",
       "      <th>cj2</th>\n",
       "      <th>cj3</th>\n",
       "      <th>srj1</th>\n",
       "      <th>srj2</th>\n",
       "      <th>srj3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>...</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>6236</td>\n",
       "      <td>2373</td>\n",
       "      <td>1498</td>\n",
       "      <td>24456</td>\n",
       "      <td>28</td>\n",
       "      <td>39</td>\n",
       "      <td>210</td>\n",
       "      <td>60631</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>8975</td>\n",
       "      <td>11894</td>\n",
       "      <td>499</td>\n",
       "      <td>155</td>\n",
       "      <td>1116</td>\n",
       "      <td>1219</td>\n",
       "      <td>1196</td>\n",
       "      <td>1230</td>\n",
       "      <td>1344</td>\n",
       "      <td>1203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>100-Concert Band</td>\n",
       "      <td>Allen High School</td>\n",
       "      <td>TEA:</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>Joe Martinez</td>\n",
       "      <td>CC</td>\n",
       "      <td>Varsity</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Colliding Visions  (Balmages/ )</td>\n",
       "      <td>Moscow, 1941  (Balmages)</td>\n",
       "      <td>DATE of EVENT 04/17/2018</td>\n",
       "      <td>Region: 8</td>\n",
       "      <td>1. Keith Bearden</td>\n",
       "      <td>2. Cindy Lansford</td>\n",
       "      <td>3. Randy Vaughn</td>\n",
       "      <td>1. Phil Anthony</td>\n",
       "      <td>2. Tye Ann Payne</td>\n",
       "      <td>3. Rick Yancey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>60803</td>\n",
       "      <td>112</td>\n",
       "      <td>56877</td>\n",
       "      <td>2882</td>\n",
       "      <td>29</td>\n",
       "      <td>14895</td>\n",
       "      <td>31202</td>\n",
       "      <td>3274</td>\n",
       "      <td>78</td>\n",
       "      <td>25874</td>\n",
       "      <td>...</td>\n",
       "      <td>422</td>\n",
       "      <td>446</td>\n",
       "      <td>714</td>\n",
       "      <td>3135</td>\n",
       "      <td>709</td>\n",
       "      <td>463</td>\n",
       "      <td>843</td>\n",
       "      <td>448</td>\n",
       "      <td>341</td>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Event              School    TEA          City  \\\n",
       "count               60803               60803  60803         60803   \n",
       "unique                  1                6236   2373          1498   \n",
       "top      100-Concert Band   Allen High School   TEA:   San Antonio   \n",
       "freq                60803                 112  56877          2882   \n",
       "\n",
       "            Directors Conference Classification   Year     ID  Stage Judge 1  \\\n",
       "count           60803      60803          60803  60803  60803          60803   \n",
       "unique          24456         28             39    210  60631             13   \n",
       "top      Joe Martinez         CC       Varsity    2019      1              1   \n",
       "freq               29      14895          31202   3274     78          25874   \n",
       "\n",
       "        ...                       Selection 2                Selection 3  \\\n",
       "count   ...                             60803                      60803   \n",
       "unique  ...                              8975                      11894   \n",
       "top     ...   Colliding Visions  (Balmages/ )   Moscow, 1941  (Balmages)   \n",
       "freq    ...                               422                        446   \n",
       "\n",
       "                             Date     Region               cj1  \\\n",
       "count                       60803      60803             60803   \n",
       "unique                        499        155              1116   \n",
       "top     DATE of EVENT 04/17/2018   Region: 8  1. Keith Bearden   \n",
       "freq                          714       3135               709   \n",
       "\n",
       "                      cj2              cj3             srj1              srj2  \\\n",
       "count               60803            60803            60803             60803   \n",
       "unique               1219             1196             1230              1344   \n",
       "top     2. Cindy Lansford  3. Randy Vaughn  1. Phil Anthony  2. Tye Ann Payne   \n",
       "freq                  463              843              448               341   \n",
       "\n",
       "                  srj3  \n",
       "count            60625  \n",
       "unique            1203  \n",
       "top     3. Rick Yancey  \n",
       "freq               736  \n",
       "\n",
       "[4 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select rows with acc column \n",
    "oops_df = df[df['oops'].notnull()]\n",
    "\n",
    "# delete the acc columns and fix column names\n",
    "oops_fix_df = oops_df.drop(columns=['Conference'])\n",
    "oops_fix_df.columns = columns_21\n",
    "\n",
    "# drop acc rows from df\n",
    "df = df.loc[df['oops'].isnull() == True]\n",
    "\n",
    "# add fixed df to df\n",
    "df = pd.concat([df, oops_fix_df], ignore_index=True)\n",
    "\n",
    "# drop oops column\n",
    "df.drop(columns=['oops'], inplace=True)\n",
    "\n",
    "# drop rows where Event contains '9'\n",
    "df = df[df['Event'].str.contains('9') == False]\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60803"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select rows where selection 3 is null\n",
    "df_clean = df\n",
    "df_clean = df_clean[df_clean['Selection 3'].isnull() == False]\n",
    "len(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim whitespace, double spaces, commas, and periods from selections\n",
    "selection_columns = ['Selection 1', 'Selection 2', 'Selection 3']\n",
    "\n",
    "for i in selection_columns:\n",
    "    df_clean[i] = df_clean[i].str.strip()\n",
    "    df_clean[i] = df_clean[i].str.replace('  ', ' ', regex=False)\n",
    "    df_clean[i] = df_clean[i].str.replace(',', '', regex=False)\n",
    "    df_clean[i] = df_clean[i].str.replace('.', '', regex=False)\n",
    "\n",
    "# Trim whitespace from classification column\n",
    "df_clean['Classification'] = df_clean['Classification'].str.strip()\n",
    "\n",
    "# Remove composer/arranger information from selections\n",
    "for column in selection_columns:\n",
    "    # remove all inside parenthesis\n",
    "    df_clean[column] = df_clean[column].str.replace('\\(.*\\)', '', regex=True)\n",
    "    # trim whitespace\n",
    "    df_clean[column] = df_clean[column].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where conference contains 'Acc'\n",
    "df_clean = df_clean[df_clean['Conference'].str.contains('Acc') == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix names of some conferences\n",
    "df_clean['Conference'] = df_clean['Conference'].replace('2C', 'CC')\n",
    "df_clean['Conference'] = df_clean['Conference'].replace('4A', 'AAAA')\n",
    "df_clean['Conference'] = df_clean['Conference'].replace('cc', 'CC')\n",
    "df_clean['Conference'] = df_clean['Conference'].replace('1C', 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make df_clean column integers\n",
    "df_clean['Year'] = df_clean['Year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Varsity ' 'CC' 'AAA' 'C' 'CCC' 'AAAA' 'AA' 'A' 'AAAAA' 'Non-Varsity '\n",
      " 'Sub Non-Varsity ' 'Sub Non-Varsity B' 'BBB' 'Non-Varsity A' 'BB' 'B'\n",
      " 'Var-Composite ' ' ' 'Non-Varsity C' 'NVar-Composite ' 'Var-Combined '\n",
      " 'AAAAAA']\n",
      "['2005' 'Varsity' 'Non-Varsity' 'Non-Varsity A' 'Non-Varsity B' 'Combined'\n",
      " 'Sub Non-Varsity' 'Sub Non-Varsity C' 'Sub Non-Varsity B'\n",
      " 'Sub Non-Varsity A' 'Non-Varsity C' 'Non-Varsity F' 'Sub Non-Varsity D'\n",
      " 'Sub Non-Varsity E' 'Non-Varsity E' '2006' 'Non-Varsity D' '2008'\n",
      " 'Var-Combined' 'Var-Composite' '' '2009' 'NVar-Composite' '2010' '2011'\n",
      " '2012' 'NVar-Combined' '2013' 'Sub Non-Varsity F' '2014' 'Varsity A'\n",
      " 'Varsity C' 'Varsity B' '2015' 'Sub Non-Varsity G' 'Var-Composite A'\n",
      " 'Sub Non-Varsity H' 'Sub Non-Varsity I' 'NVar-Composite A']\n",
      "[ 17246   2005  14077  11197  11109  17900  16404  17069  17328  13344\n",
      "  15094  13014  17315  17648  18475  18405  17673  14770  13205  13902\n",
      "  15470  15934  13485  16213  15350  15679  15324  15314  15307  15306\n",
      "  15299  15286  15285  15659  13687  14327  12811  12205   2006  11014\n",
      "   2007   2008  35160   2009  47560  45191  45104  45182  46485  44292\n",
      "  50328  47999  48920  50759  44294  48170  46386  49872  50493  48661\n",
      "  49145  46108  50260   2010  56078  56241  54936  53910  52795  54505\n",
      "  53020  55434  56728  53211  53302  58047  58342  57702  56477  53145\n",
      "  58874  58840  59065  59378  53787  55213  54701  58418  58747  58606\n",
      "  57719  59132  59178  58033  59340  58344  61095   2011  62049  66236\n",
      "  64268  64262  63170  65547  67406  66910  65191  66015  62734  59940\n",
      "  65738  65174  60625  66830  61502  66992  67830  63744  65785  59905\n",
      "  68247  62267  63683  62437  62047  67275  64462  63858  67595  64219\n",
      "  67662   2012  68890  71856  74197  76620  71735  71456  71285  71124\n",
      "  71009  70932  69702  68769  69385  70987  69750  73839  76805  68896\n",
      "  70327  75301  75363  75360  72065  75163  73474  72383  75952  75552\n",
      "  74594  74464  76594  72848  73458   2013  84757  85265  81523  85468\n",
      "  77561  81653  86165  86440  80464  86288  84039  77679   2014  92973\n",
      "  95724  91945  86994  88369  87217  92418  92415   2015 104122   2016\n",
      "   2017   2018   2019   2020   2021   2022]\n"
     ]
    }
   ],
   "source": [
    "print(df_clean['Conference'].unique())\n",
    "print(df_clean['Classification'].unique())\n",
    "print(df_clean['Year'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "judging_columns = ['Stage Judge 1', 'Stage Judge 2', 'Stage Judge 3', 'Stage Final', 'SR Judge 1', \n",
    "                'SR Judge 2', 'SR Judge 3', 'SR Final']\n",
    "numbers = ['1', '2', '3', '4', '5']\n",
    "\n",
    "for n in numbers:\n",
    "    for j in judging_columns:\n",
    "        df_clean.loc[df_clean[j] == n, j] = int(n)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ' 2 1 3 4 'DNA' 5 'DQ']\n",
      "[' ' 2 1 4 3 'DNA' 5 'DQ']\n",
      "[' ' 2 3 1 4 'DNA' 'DQ' 5]\n",
      "[' ' 2 1 3 4 'DNA' 5 'DQ']\n",
      "[' ' 1 2 3 4 5 'DNA' 'DQ']\n",
      "[' ' 1 2 3 4 'DNA' 5 'DQ']\n",
      "[' ' 1 2 3 4 'DNA' 5 'DQ']\n",
      "[' ' 1 2 3 4 'DNA' 5 'TD' 'TRC' 'C' 'DQ' 'PLQ' 'TRO' 'RM1' 'RMD' 'RM2'\n",
      " 'RMC' 'A' '-' 11 'SWA' 'NAN' 'B' 'D' 0]\n"
     ]
    }
   ],
   "source": [
    "for j in judging_columns:\n",
    "    print(df_clean[j].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim whitespace, double spaces, commas, and periods from selections\n",
    "selection_columns = ['Selection 1', 'Selection 2', 'Selection 3']\n",
    "\n",
    "for i in selection_columns:\n",
    "    df_clean[i] = df_clean[i].str.strip()\n",
    "    df_clean[i] = df_clean[i].str.replace('  ', ' ', regex=False)\n",
    "    df_clean[i] = df_clean[i].str.replace(',', '', regex=False)\n",
    "    df_clean[i] = df_clean[i].str.replace('.', '', regex=False)\n",
    "\n",
    "# Trim whitespace from classification column\n",
    "df_clean['Classification'] = df_clean['Classification'].str.strip()\n",
    "\n",
    "# Remove composer/arranger information from selections\n",
    "for column in selection_columns:\n",
    "    # remove all inside parenthesis\n",
    "    df_clean[column] = df_clean[column].str.replace('\\(.*\\)', '', regex=True)\n",
    "    # trim whitespace\n",
    "    df_clean[column] = df_clean[column].str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.columns = df_clean.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60803\n"
     ]
    }
   ],
   "source": [
    "# Drop Event and TEA column\n",
    "df = df.drop(columns=['Event', 'TEA'])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60269\n"
     ]
    }
   ],
   "source": [
    "# Remove DNA, DQ\n",
    "for i in judging_columns:\n",
    "    df_clean = df_clean[df_clean[i] != 'DNA']\n",
    "    df_clean = df_clean[df_clean[i] != 'DQ']\n",
    "\n",
    "# Convert blanks to nans\n",
    "for i in judging_columns:\n",
    "    df_clean[i] = df_clean[i].replace(['', ' '], np.nan)\n",
    "\n",
    "print(len(df_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average together Stage Judge 1, 2, and 3 into a new column\n",
    "\n",
    "df_clean['Stage Average'] = (df_clean['Stage Judge 1'] + df_clean['Stage Judge 2'] + df_clean['Stage Judge 3']) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort df_clean by year\n",
    "df_clean.sort_values(by=['Year'], inplace=True)\n",
    "\n",
    "# drop Event column\n",
    "df_clean = df_clean.drop(columns=['Event'])\n",
    "\n",
    "# drop TEA column\n",
    "df_clean = df_clean.drop(columns=['TEA'])\n",
    "\n",
    "# drop ID column\n",
    "df_clean = df_clean.drop(columns=['ID'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all of the pml csv files into a dataframe\n",
    "files = glob.glob(\"csv_files/pml/*.csv\")\n",
    "pml_df = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
    "# convert pml_df to csv\n",
    "pml_df.to_csv(\"csv_files/pml_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['Date'] = df_clean['Date'].str.replace('DATE of EVENT ', '')\n",
    "judge_name_columns = ['cj1', 'cj2', 'cj3', 'srj1', 'srj2', 'srj3']\n",
    "for i in judge_name_columns:\n",
    "    df_clean[i] = df_clean[i].str.replace('1. ', '', regex=False)\n",
    "    df_clean[i] = df_clean[i].str.replace('2. ', '', regex=False)\n",
    "    df_clean[i] = df_clean[i].str.replace('3. ', '', regex=False)\n",
    "    # all lowercase\n",
    "    df_clean[i] = df_clean[i].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each judge name column, remove everything after the first comma\n",
    "for i in judge_name_columns:\n",
    "    #df_clean[i] = df_clean[i].str.split(',', expand=True)[0]\n",
    "    #df_clean[i] = df_clean[i].str.split('-', expand=True)[0]\n",
    "    # trim whitespace\n",
    "    df_clean[i] = df_clean[i].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = df_clean.columns\n",
    "# trim whitespace from all columns\n",
    "for i in all_columns:\n",
    "    try:\n",
    "        df_clean[i] = df_clean[i].str.strip()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where School = 0\n",
    "df_clean = df_clean[df_clean['School'] != '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rows where year is greater than 3000\n",
    "df_clean_yr_error = df_clean[df_clean['Year'] > 3000]\n",
    "\n",
    "# drop rows where year is greater than 3000\n",
    "df_clean = df_clean[df_clean['Year'] < 3000]\n",
    "\n",
    "df_clean_yr_error['Year'] = df_clean_yr_error['Classification']\n",
    "df_clean_yr_error['Classification'] = df_clean_yr_error['Conference']\n",
    "\n",
    "# fill conference column with blanks\n",
    "df_clean_yr_error['Conference'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine df_clean and df_clean_yr_error\n",
    "df_clean = pd.concat([df_clean, df_clean_yr_error], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAAAA' 'A' 'AA' 'AAA' 'AAAA' 'C' 'CC' 'CCC' 'BBB' 'BB' 'B' 'AAAAAA' '']\n",
      "['Non-Varsity' 'Varsity' 'Sub Non-Varsity' 'Non-Varsity A'\n",
      " 'Sub Non-Varsity B' 'Sub Non-Varsity A' 'Sub Non-Varsity C'\n",
      " 'Sub Non-Varsity E' 'Sub Non-Varsity D' 'Non-Varsity B' 'Non-Varsity E'\n",
      " 'Non-Varsity C' 'Combined' 'Non-Varsity F' 'Non-Varsity D'\n",
      " 'Var-Composite' 'Var-Combined' '' 'NVar-Composite' 'NVar-Combined'\n",
      " 'Sub Non-Varsity F' 'Varsity C' 'Varsity B' 'Varsity A'\n",
      " 'Sub Non-Varsity G' 'Var-Composite A' 'Sub Non-Varsity H'\n",
      " 'Sub Non-Varsity I' 'NVar-Composite A']\n",
      "[2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018\n",
      " 2019 2020 2021 2022 '2008' '2009' '2010' '2011' '2012' '2013' '2014'\n",
      " '2015']\n"
     ]
    }
   ],
   "source": [
    "print(df_clean['Conference'].unique())\n",
    "print(df_clean['Classification'].unique())\n",
    "print(df_clean['Year'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "School             object\n",
       "City               object\n",
       "Directors          object\n",
       "Conference         object\n",
       "Classification     object\n",
       "Year               object\n",
       "Stage Judge 1     float64\n",
       "Stage Judge 2     float64\n",
       "Stage Judge 3     float64\n",
       "Stage Final       float64\n",
       "SR Judge 1        float64\n",
       "SR Judge 2        float64\n",
       "SR Judge 3        float64\n",
       "SR Final           object\n",
       "Award              object\n",
       "Selection 1        object\n",
       "Selection 2        object\n",
       "Selection 3        object\n",
       "Date               object\n",
       "Region             object\n",
       "cj1                object\n",
       "cj2                object\n",
       "cj3                object\n",
       "srj1               object\n",
       "srj2               object\n",
       "srj3               object\n",
       "Stage Average     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>School</th>\n",
       "      <th>City</th>\n",
       "      <th>Directors</th>\n",
       "      <th>Conference</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Year</th>\n",
       "      <th>Stage Judge 1</th>\n",
       "      <th>Stage Judge 2</th>\n",
       "      <th>Stage Judge 3</th>\n",
       "      <th>Stage Final</th>\n",
       "      <th>...</th>\n",
       "      <th>Selection 3</th>\n",
       "      <th>Date</th>\n",
       "      <th>Region</th>\n",
       "      <th>cj1</th>\n",
       "      <th>cj2</th>\n",
       "      <th>cj3</th>\n",
       "      <th>srj1</th>\n",
       "      <th>srj2</th>\n",
       "      <th>srj3</th>\n",
       "      <th>Stage Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>James Bowie High School</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>Larry Brown</td>\n",
       "      <td>AAAAA</td>\n",
       "      <td>Non-Varsity</td>\n",
       "      <td>2005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Festivo</td>\n",
       "      <td>04/06/2005</td>\n",
       "      <td>Region: 5</td>\n",
       "      <td>richard bass</td>\n",
       "      <td>joe frank, jr.</td>\n",
       "      <td>rodney klett</td>\n",
       "      <td>george jones</td>\n",
       "      <td>tom neugent</td>\n",
       "      <td>marion west</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Munday High School</td>\n",
       "      <td>Munday</td>\n",
       "      <td>Rodney D. Bennett</td>\n",
       "      <td>A</td>\n",
       "      <td>Varsity</td>\n",
       "      <td>2005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Flight of Valor</td>\n",
       "      <td>03/10/2005</td>\n",
       "      <td>Region: 2</td>\n",
       "      <td>mike glaze</td>\n",
       "      <td>richard herrera</td>\n",
       "      <td>will burks</td>\n",
       "      <td>harold bufe</td>\n",
       "      <td>mack bibb</td>\n",
       "      <td>june bearden</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Holliday High School</td>\n",
       "      <td>Holliday</td>\n",
       "      <td>Melanie Hadderton</td>\n",
       "      <td>AA</td>\n",
       "      <td>Varsity</td>\n",
       "      <td>2005</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Pablo!</td>\n",
       "      <td>03/10/2005</td>\n",
       "      <td>Region: 2</td>\n",
       "      <td>mike glaze</td>\n",
       "      <td>richard herrera</td>\n",
       "      <td>will burks</td>\n",
       "      <td>harold bufe</td>\n",
       "      <td>mack bibb</td>\n",
       "      <td>june bearden</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decatur High School</td>\n",
       "      <td>Decatur</td>\n",
       "      <td>Doug Fulwood</td>\n",
       "      <td>AAA</td>\n",
       "      <td>Varsity</td>\n",
       "      <td>2005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>West Highlands Sojourn</td>\n",
       "      <td>03/10/2005</td>\n",
       "      <td>Region: 2</td>\n",
       "      <td>mike glaze</td>\n",
       "      <td>richard herrera</td>\n",
       "      <td>will burks</td>\n",
       "      <td>harold bufe</td>\n",
       "      <td>mack bibb</td>\n",
       "      <td>june bearden</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>City View High School</td>\n",
       "      <td>Wichita Falls</td>\n",
       "      <td>Terah Kay Shawver</td>\n",
       "      <td>AA</td>\n",
       "      <td>Varsity</td>\n",
       "      <td>2005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Canterbury Walk</td>\n",
       "      <td>03/10/2005</td>\n",
       "      <td>Region: 2</td>\n",
       "      <td>mike glaze</td>\n",
       "      <td>richard herrera</td>\n",
       "      <td>will burks</td>\n",
       "      <td>harold bufe</td>\n",
       "      <td>mack bibb</td>\n",
       "      <td>june bearden</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60225</th>\n",
       "      <td>Alvarado MS</td>\n",
       "      <td>Alvarado</td>\n",
       "      <td>Kelli Bahner / Joe Gunn</td>\n",
       "      <td></td>\n",
       "      <td>Non-Varsity</td>\n",
       "      <td>2014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>DATE of EVENT 03/25/2014</td>\n",
       "      <td>Region: 7</td>\n",
       "      <td>1. Bryan Bronstad</td>\n",
       "      <td>christine cumberledge</td>\n",
       "      <td>james marioneaux</td>\n",
       "      <td>julie amos</td>\n",
       "      <td>corey ash</td>\n",
       "      <td>harold bufe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60226</th>\n",
       "      <td>Alvarado MS</td>\n",
       "      <td>Alvarado</td>\n",
       "      <td>Kelli Bahner / Joe Gunn</td>\n",
       "      <td></td>\n",
       "      <td>Varsity</td>\n",
       "      <td>2014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>DATE of EVENT 03/25/2014</td>\n",
       "      <td>Region: 7</td>\n",
       "      <td>1. Bryan Bronstad</td>\n",
       "      <td>christine cumberledge</td>\n",
       "      <td>james marioneaux</td>\n",
       "      <td>julie amos</td>\n",
       "      <td>corey ash</td>\n",
       "      <td>harold bufe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60227</th>\n",
       "      <td>Del Rio High School</td>\n",
       "      <td>Del Rio</td>\n",
       "      <td>Daniel White</td>\n",
       "      <td></td>\n",
       "      <td>Sub Non-Varsity</td>\n",
       "      <td>2014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>DATE of EVENT 04/10/2014</td>\n",
       "      <td>Region: 11</td>\n",
       "      <td>1. John Faraone</td>\n",
       "      <td>bob whipkey</td>\n",
       "      <td>rogerio olivarez</td>\n",
       "      <td>charles cabrera</td>\n",
       "      <td>kyle friesenhahn</td>\n",
       "      <td>juan sosa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60228</th>\n",
       "      <td>Stinson Middle School</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>Kevin Leman / Alex Melendez</td>\n",
       "      <td></td>\n",
       "      <td>Non-Varsity</td>\n",
       "      <td>2014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>DATE of EVENT 04/16/2014</td>\n",
       "      <td>Region: 11</td>\n",
       "      <td>1. Penny Compton</td>\n",
       "      <td>kim rosenberg</td>\n",
       "      <td>javier vera</td>\n",
       "      <td>james snider</td>\n",
       "      <td>cathy teltschik</td>\n",
       "      <td>larry wolf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60229</th>\n",
       "      <td>Del Rio High School</td>\n",
       "      <td>Del Rio</td>\n",
       "      <td>Daniel White</td>\n",
       "      <td></td>\n",
       "      <td>Non-Varsity</td>\n",
       "      <td>2015</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>DATE of EVENT 04/07/2015</td>\n",
       "      <td>Region: 11</td>\n",
       "      <td>1. Jeff Lightsey</td>\n",
       "      <td>ronnie rios</td>\n",
       "      <td>james snider</td>\n",
       "      <td>alfonso alvarado</td>\n",
       "      <td>john faraone</td>\n",
       "      <td>roger olivarez</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60230 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        School           City                    Directors  \\\n",
       "0      James Bowie High School      Arlington                  Larry Brown   \n",
       "1           Munday High School         Munday            Rodney D. Bennett   \n",
       "2         Holliday High School       Holliday            Melanie Hadderton   \n",
       "3          Decatur High School        Decatur                 Doug Fulwood   \n",
       "4        City View High School  Wichita Falls            Terah Kay Shawver   \n",
       "...                        ...            ...                          ...   \n",
       "60225              Alvarado MS       Alvarado      Kelli Bahner / Joe Gunn   \n",
       "60226              Alvarado MS       Alvarado      Kelli Bahner / Joe Gunn   \n",
       "60227      Del Rio High School        Del Rio                 Daniel White   \n",
       "60228    Stinson Middle School    San Antonio  Kevin Leman / Alex Melendez   \n",
       "60229      Del Rio High School        Del Rio                 Daniel White   \n",
       "\n",
       "      Conference   Classification  Year  Stage Judge 1  Stage Judge 2  \\\n",
       "0          AAAAA      Non-Varsity  2005            1.0            1.0   \n",
       "1              A          Varsity  2005            1.0            1.0   \n",
       "2             AA          Varsity  2005            2.0            2.0   \n",
       "3            AAA          Varsity  2005            1.0            2.0   \n",
       "4             AA          Varsity  2005            1.0            1.0   \n",
       "...          ...              ...   ...            ...            ...   \n",
       "60225                 Non-Varsity  2014            1.0            3.0   \n",
       "60226                     Varsity  2014            1.0            1.0   \n",
       "60227             Sub Non-Varsity  2014            1.0            1.0   \n",
       "60228                 Non-Varsity  2014            1.0            1.0   \n",
       "60229                 Non-Varsity  2015            2.0            2.0   \n",
       "\n",
       "       Stage Judge 3  Stage Final  ...               Selection 3        Date  \\\n",
       "0                1.0          1.0  ...                   Festivo  04/06/2005   \n",
       "1                2.0          1.0  ...           Flight of Valor  03/10/2005   \n",
       "2                3.0          2.0  ...                    Pablo!  03/10/2005   \n",
       "3                2.0          2.0  ...    West Highlands Sojourn  03/10/2005   \n",
       "4                1.0          1.0  ...           Canterbury Walk  03/10/2005   \n",
       "...              ...          ...  ...                       ...         ...   \n",
       "60225            1.0          2.0  ...  DATE of EVENT 03/25/2014   Region: 7   \n",
       "60226            1.0          1.0  ...  DATE of EVENT 03/25/2014   Region: 7   \n",
       "60227            1.0          1.0  ...  DATE of EVENT 04/10/2014  Region: 11   \n",
       "60228            1.0          1.0  ...  DATE of EVENT 04/16/2014  Region: 11   \n",
       "60229            2.0          1.0  ...  DATE of EVENT 04/07/2015  Region: 11   \n",
       "\n",
       "                  Region                    cj1               cj2  \\\n",
       "0              Region: 5           richard bass    joe frank, jr.   \n",
       "1              Region: 2             mike glaze   richard herrera   \n",
       "2              Region: 2             mike glaze   richard herrera   \n",
       "3              Region: 2             mike glaze   richard herrera   \n",
       "4              Region: 2             mike glaze   richard herrera   \n",
       "...                  ...                    ...               ...   \n",
       "60225  1. Bryan Bronstad  christine cumberledge  james marioneaux   \n",
       "60226  1. Bryan Bronstad  christine cumberledge  james marioneaux   \n",
       "60227    1. John Faraone            bob whipkey  rogerio olivarez   \n",
       "60228   1. Penny Compton          kim rosenberg       javier vera   \n",
       "60229   1. Jeff Lightsey            ronnie rios      james snider   \n",
       "\n",
       "                    cj3              srj1            srj2          srj3  \\\n",
       "0          rodney klett      george jones     tom neugent   marion west   \n",
       "1            will burks       harold bufe       mack bibb  june bearden   \n",
       "2            will burks       harold bufe       mack bibb  june bearden   \n",
       "3            will burks       harold bufe       mack bibb  june bearden   \n",
       "4            will burks       harold bufe       mack bibb  june bearden   \n",
       "...                 ...               ...             ...           ...   \n",
       "60225        julie amos         corey ash     harold bufe           NaN   \n",
       "60226        julie amos         corey ash     harold bufe           NaN   \n",
       "60227   charles cabrera  kyle friesenhahn       juan sosa           NaN   \n",
       "60228      james snider   cathy teltschik      larry wolf           NaN   \n",
       "60229  alfonso alvarado      john faraone  roger olivarez           NaN   \n",
       "\n",
       "      Stage Average  \n",
       "0          1.000000  \n",
       "1          1.333333  \n",
       "2          2.333333  \n",
       "3          1.666667  \n",
       "4          1.000000  \n",
       "...             ...  \n",
       "60225      1.666667  \n",
       "60226      1.000000  \n",
       "60227      1.000000  \n",
       "60228      1.000000  \n",
       "60229      2.000000  \n",
       "\n",
       "[60230 rows x 27 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each column, remove all text after ' - '\n",
    "for i in judge_name_columns:\n",
    "    df_clean[i] = df_clean[i].str.split(' - ', expand=True)[0]\n",
    "    # trim whitespace\n",
    "    df_clean[i] = df_clean[i].str.strip()\n",
    "\n",
    "df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the unique values in cj1\n",
    "lolcody = df_clean['cj1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in judge_name_columns:\n",
    "    df_clean[each] = df_clean[each].str.replace(', lubbock', '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-harlingen\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", san antonio\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", sharyland\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", austin\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", moore\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", mission\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", edinburg north hs, edinburg cisd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", edinburg north hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", edinburg cisd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", lake travis\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", robstown hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", robstown\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", h.e.b.\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", duncanville\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", concert\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", sr\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", roma isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", roma\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", retired\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", corpus christi\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", la feria hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", la feria\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", iii\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", orange grove\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", weslaco\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", chair\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"(chair)\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", director fine arts\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", director of fine arts\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", tomball isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", baytown\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", ret.\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", edinburg hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", edinburg\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", juarez-lincoln hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", la joya isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", la joya\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", alvarado\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", mcallen isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", mcallen\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", united isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", laredo\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", mcqueeny\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", seguin\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", mission cisd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", connally hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", el paso\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", el paso isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", brownsville isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", brownsville\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", inst.musicadv.\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", grulla hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", rio grande city isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", rio grande city\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", midway hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", woodway\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", tuloso-midway hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", ac blunt ms\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", aransas pass\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", leander\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", falfurrias hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", falfurrias\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", kingsville\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-retired\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", weslaco isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", fine arts administrator\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\",la joya isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", aledo\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", canyon\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", p.s.j.a.\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", odem hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", c.o. wilson ms\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", garland\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", houston\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-banda\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"*\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"--\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"1\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"2\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"3\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"4\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"5\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"6\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"xxx\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"virginia osolvsky\", 'virginia olsovsky', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"virginia osovsky\", 'virginia olsovsky', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"virginian olsovsky\", 'virginia olsovsky', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"villareal\", 'villarreal', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-flour bluff\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\" d. \", ' ', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"wallace diefolf\", 'wallace dierolf', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"walace dierolf\", 'wallace dierolf', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"unknown\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"ty ann payne\", 'tye ann payne', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"tye ann payne\", 'tye payne', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-bastrop\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"(10th)\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"(11th)\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"allmany\", 'almany', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"mcelory\", 'mcelroy', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"knolficek\", 'knoflicek', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"knloficek\", 'knoflicek', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"knofllicek\", 'knoflicek', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"tom herrington\", 'tom harrington', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\" (chrmn.)\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\" (chmn)\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\" (chmn.)\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"clearwarter\", 'clearwater', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\" .j \", ' ', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"tim edens\", 'tim edins', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"tim andersen\", 'tim anderson', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"terri brockway\", 'teri brockway', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"tbd/\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"tammy fedenych\", 'tammy fedynich', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"tammy fedinich\", 'tammy fedynich', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"tammy fednmich\", 'tammy fedynich', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"susan meyer-patterson\", 'susan patterson', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"susan meyer patterson\", 'susan patterson', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-houston\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-pearsall\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-psja\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-corpus christi\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-keller\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"steel, jason\", 'steele, jason', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"maudlin\", 'mauldin', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"stacy claek\", 'stacy clark', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"stacy clark\", 'stacey clark', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"sr #1\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"sr #2\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"sr #3\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"4/11/17\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"4/13/17\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"see region 21 web site\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"see original event\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"see original contest\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"see original evenrt\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"sheppard\", 'shepherd', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-beaumont\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"sandy bow brunskill\", 'sandra brunskill', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"sandy brunskill\", 'sandra brunskill', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"sandra bow brunskill\", 'sandra brunskill', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"sandra bow-brunskill\", 'sandra brunskill', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"rylon guidory\", 'rylon guidry', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-austin\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"ryan straten\", 'ryan stratten', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-mabank\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"rusty honeycutt\", 'rustin honeycutt', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"barerra\", 'barrera', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"costellano\", 'castellano', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\" m. \", ' ', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\",san antonio\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", frisco\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"/poteet\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\" (8th)\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\" (9th)\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-spring\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"margarit\", 'margaret', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"nv-\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"/v-\", '/', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"//\", '/', regex=False)\n",
    "    #df_clean[each] = df_clean[each].str.replace(\"^x?\", '', regex=True)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"^tba?\", '', regex=True)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"^tbd?\", '', regex=True)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"- -\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-plano\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\" .k \", ' ', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"bene davis \", 'ben davis', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"benny davis\", 'ben davis', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"see region website\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"see region web site\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"rick yancy\", 'rick yancey', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"phillip alvarado\", 'phil alvarado', regex=False)\n",
    "    #df_clean[each] = df_clean[each].str.replace(\"^d?\", '', regex=True)\n",
    "    #df_clean[each] = df_clean[each].str.replace(\"^f?\", '', regex=True)\n",
    "    #df_clean[each] = df_clean[each].str.replace(\"^judge?\", '', regex=True)\n",
    "    #df_clean[each] = df_clean[each].str.replace(\"^b?\", '', regex=True)\n",
    "    #df_clean[each] = df_clean[each].str.replace(\"^c?\", '', regex=True)\n",
    "    #df_clean[each] = df_clean[each].str.replace(\"^a?\", '', regex=True)\n",
    "    #df_clean[each] = df_clean[each].str.replace(\"^e?\", '', regex=True)\n",
    "    #df_clean[each] = df_clean[each].str.replace(\"^t?\", '', regex=True)\n",
    "\n",
    "    df_clean[each] = df_clean[each].str.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to csv\n",
    "df_clean.to_csv(\"csv_files/full_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53babf5c189ad8a4933a2bd5063a1840a2c47c9b82a6c06a0d6cefa0137ac8c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
