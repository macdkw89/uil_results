{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import fuzzywuzzy as fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define column headers for csv export\n",
    "columns_21 = ['Event', 'School', 'TEA', 'City', 'Directors', 'Conference', 'Classification', 'Year', 'ID', \n",
    "    'Stage Judge 1', 'Stage Judge 2', 'Stage Judge 3', 'Stage Final', \n",
    "    'SR Judge 1', 'SR Judge 2', 'SR Judge 3', 'SR Final', 'Award', \n",
    "    'Selection 1', 'Selection 2', 'Selection 3', 'Date', 'Region', 'cj1', 'cj2', 'cj3', 'srj1', 'srj2', 'srj3']\n",
    "columns_22 = ['Event', 'School', 'TEA', 'City', 'Directors', 'Conference', 'Classification', 'Year', 'ID', \n",
    "    'Stage Judge 1', 'Stage Judge 2', 'Stage Judge 3', 'Stage Final', \n",
    "    'SR Judge 1', 'SR Judge 2', 'SR Judge 3', 'SR Final', 'Award', \n",
    "    'Selection 1', 'Selection 2', 'Selection 3', 'Date', 'Region', 'cj1', 'cj2', 'cj3', 'srj1', 'srj2', 'srj3', 'oops']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group the CSVs together, and fix rows with Accompanist error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of merged files returned\n",
    "files = glob.glob(\"full_run/*.csv\")\n",
    "\n",
    "# joining files with concat and read_csv\n",
    "df = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# drop unnamed column\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# name the columns\n",
    "df.columns = columns_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event</th>\n",
       "      <th>School</th>\n",
       "      <th>TEA</th>\n",
       "      <th>City</th>\n",
       "      <th>Directors</th>\n",
       "      <th>Conference</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Year</th>\n",
       "      <th>ID</th>\n",
       "      <th>Stage Judge 1</th>\n",
       "      <th>...</th>\n",
       "      <th>Selection 2</th>\n",
       "      <th>Selection 3</th>\n",
       "      <th>Date</th>\n",
       "      <th>Region</th>\n",
       "      <th>cj1</th>\n",
       "      <th>cj2</th>\n",
       "      <th>cj3</th>\n",
       "      <th>srj1</th>\n",
       "      <th>srj2</th>\n",
       "      <th>srj3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>...</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60803</td>\n",
       "      <td>60625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>6236</td>\n",
       "      <td>2373</td>\n",
       "      <td>1498</td>\n",
       "      <td>24456</td>\n",
       "      <td>28</td>\n",
       "      <td>39</td>\n",
       "      <td>210</td>\n",
       "      <td>60631</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>8975</td>\n",
       "      <td>11894</td>\n",
       "      <td>499</td>\n",
       "      <td>155</td>\n",
       "      <td>1116</td>\n",
       "      <td>1219</td>\n",
       "      <td>1196</td>\n",
       "      <td>1230</td>\n",
       "      <td>1344</td>\n",
       "      <td>1203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>100-Concert Band</td>\n",
       "      <td>Allen High School</td>\n",
       "      <td>TEA:</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>Joe Martinez</td>\n",
       "      <td>CC</td>\n",
       "      <td>Varsity</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Colliding Visions  (Balmages/ )</td>\n",
       "      <td>Moscow, 1941  (Balmages)</td>\n",
       "      <td>DATE of EVENT 04/17/2018</td>\n",
       "      <td>Region: 8</td>\n",
       "      <td>1. Keith Bearden</td>\n",
       "      <td>2. Cindy Lansford</td>\n",
       "      <td>3. Randy Vaughn</td>\n",
       "      <td>1. Phil Anthony</td>\n",
       "      <td>2. Tye Ann Payne</td>\n",
       "      <td>3. Rick Yancey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>60803</td>\n",
       "      <td>112</td>\n",
       "      <td>56877</td>\n",
       "      <td>2882</td>\n",
       "      <td>29</td>\n",
       "      <td>14895</td>\n",
       "      <td>31202</td>\n",
       "      <td>3274</td>\n",
       "      <td>78</td>\n",
       "      <td>25874</td>\n",
       "      <td>...</td>\n",
       "      <td>422</td>\n",
       "      <td>446</td>\n",
       "      <td>714</td>\n",
       "      <td>3135</td>\n",
       "      <td>709</td>\n",
       "      <td>463</td>\n",
       "      <td>843</td>\n",
       "      <td>448</td>\n",
       "      <td>341</td>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Event              School    TEA          City  \\\n",
       "count               60803               60803  60803         60803   \n",
       "unique                  1                6236   2373          1498   \n",
       "top      100-Concert Band   Allen High School   TEA:   San Antonio   \n",
       "freq                60803                 112  56877          2882   \n",
       "\n",
       "            Directors Conference Classification   Year     ID  Stage Judge 1  \\\n",
       "count           60803      60803          60803  60803  60803          60803   \n",
       "unique          24456         28             39    210  60631             13   \n",
       "top      Joe Martinez         CC       Varsity    2019      1              1   \n",
       "freq               29      14895          31202   3274     78          25874   \n",
       "\n",
       "        ...                       Selection 2                Selection 3  \\\n",
       "count   ...                             60803                      60803   \n",
       "unique  ...                              8975                      11894   \n",
       "top     ...   Colliding Visions  (Balmages/ )   Moscow, 1941  (Balmages)   \n",
       "freq    ...                               422                        446   \n",
       "\n",
       "                             Date     Region               cj1  \\\n",
       "count                       60803      60803             60803   \n",
       "unique                        499        155              1116   \n",
       "top     DATE of EVENT 04/17/2018   Region: 8  1. Keith Bearden   \n",
       "freq                          714       3135               709   \n",
       "\n",
       "                      cj2              cj3             srj1              srj2  \\\n",
       "count               60803            60803            60803             60803   \n",
       "unique               1219             1196             1230              1344   \n",
       "top     2. Cindy Lansford  3. Randy Vaughn  1. Phil Anthony  2. Tye Ann Payne   \n",
       "freq                  463              843              448               341   \n",
       "\n",
       "                  srj3  \n",
       "count            60625  \n",
       "unique            1203  \n",
       "top     3. Rick Yancey  \n",
       "freq               736  \n",
       "\n",
       "[4 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select rows with acc column \n",
    "oops_df = df[df['oops'].notnull()]\n",
    "\n",
    "# delete the acc columns and fix column names\n",
    "oops_fix_df = oops_df.drop(columns=['Conference'])\n",
    "oops_fix_df.columns = columns_21\n",
    "\n",
    "# drop acc rows from df\n",
    "df = df.loc[df['oops'].isnull() == True]\n",
    "\n",
    "# add fixed df to df\n",
    "df = pd.concat([df, oops_fix_df], ignore_index=True)\n",
    "\n",
    "# drop oops column\n",
    "df.drop(columns=['oops'], inplace=True)\n",
    "\n",
    "# drop rows where Event contains '9'\n",
    "df = df[df['Event'].str.contains('9') == False]\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60803"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select rows where selection 3 is null\n",
    "df_clean = df\n",
    "df_clean = df_clean[df_clean['Selection 3'].isnull() == False]\n",
    "len(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim whitespace, double spaces, commas, and periods from selections\n",
    "selection_columns = ['Selection 1', 'Selection 2', 'Selection 3']\n",
    "\n",
    "for i in selection_columns:\n",
    "    df_clean[i] = df_clean[i].str.strip()\n",
    "    df_clean[i] = df_clean[i].str.replace('  ', ' ', regex=False)\n",
    "    df_clean[i] = df_clean[i].str.replace(',', '', regex=False)\n",
    "    df_clean[i] = df_clean[i].str.replace('.', '', regex=False)\n",
    "\n",
    "# Trim whitespace from classification column\n",
    "df_clean['Classification'] = df_clean['Classification'].str.strip()\n",
    "\n",
    "# Remove composer/arranger information from selections\n",
    "for column in selection_columns:\n",
    "    # remove all inside parenthesis\n",
    "    df_clean[f\"{column} Comp/arr\"] = df_clean[column].str.extract(r'\\((.*?)\\)', expand=False)\n",
    "    df_clean[column] = df_clean[column].str.replace('\\(.*\\)', '', regex=True)\n",
    "    # trim whitespace\n",
    "    df_clean[column] = df_clean[column].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where conference contains 'Acc'\n",
    "df_clean = df_clean[df_clean['Conference'].str.contains('Acc') == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix names of some conferences\n",
    "df_clean['Conference'] = df_clean['Conference'].replace('2C', 'CC')\n",
    "df_clean['Conference'] = df_clean['Conference'].replace('4A', 'AAAA')\n",
    "df_clean['Conference'] = df_clean['Conference'].replace('cc', 'CC')\n",
    "df_clean['Conference'] = df_clean['Conference'].replace('1C', 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make df_clean column integers\n",
    "df_clean['Year'] = df_clean['Year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "judging_columns = ['Stage Judge 1', 'Stage Judge 2', 'Stage Judge 3', 'Stage Final', 'SR Judge 1', \n",
    "                'SR Judge 2', 'SR Judge 3', 'SR Final']\n",
    "numbers = ['1', '2', '3', '4', '5']\n",
    "\n",
    "for n in numbers:\n",
    "    for j in judging_columns:\n",
    "        df_clean.loc[df_clean[j] == n, j] = int(n)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim whitespace, double spaces, commas, and periods from selections\n",
    "selection_columns = ['Selection 1', 'Selection 2', 'Selection 3']\n",
    "\n",
    "for i in selection_columns:\n",
    "    df_clean[i] = df_clean[i].str.strip()\n",
    "    df_clean[i] = df_clean[i].str.replace('  ', ' ', regex=False)\n",
    "    df_clean[i] = df_clean[i].str.replace(',', '', regex=False)\n",
    "    df_clean[i] = df_clean[i].str.replace('.', '', regex=False)\n",
    "\n",
    "# Trim whitespace from classification column\n",
    "df_clean['Classification'] = df_clean['Classification'].str.strip()\n",
    "\n",
    "# Remove composer/arranger information from selections\n",
    "for column in selection_columns:\n",
    "    # remove all inside parenthesis\n",
    "    df_clean[column] = df_clean[column].str.replace('\\(.*\\)', '', regex=True)\n",
    "    # trim whitespace\n",
    "    df_clean[column] = df_clean[column].str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.columns = df_clean.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60803\n"
     ]
    }
   ],
   "source": [
    "# Drop Event and TEA column\n",
    "df = df.drop(columns=['Event', 'TEA'])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['School'] = df_clean['School'].str.replace(\"Junior High School\", 'JH', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"Junior High\", 'JH', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"JuniorHigh\", 'JH', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"JH School\", 'JH', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"High School\", 'HS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"high School\", 'HS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"High school\", 'HS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"high school\", 'HS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"Middle School\", 'MS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"HighSchool\", 'HS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"MiddleSchool\", 'MS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"Intermediate School\", 'IS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"Intermediat School\", 'IS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"Midle School\", 'MS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"Midddle School\", 'MS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"MS School\", 'MS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"M.S. School\", 'MS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"HIgh School\", 'HS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"MIddle School\", 'MS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"Middle 7 School\", 'MS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"Higjh School\", 'HS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"Niddle School\", 'MS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"middles School\", 'MS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"H.S.\", 'HS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"M.S.\", 'MS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"Middle SChool\", 'MS', regex=True)\n",
    "df_clean['School'] = df_clean['School'].str.replace(\"HS Band\", 'HS', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count         60801\n",
       "unique         4712\n",
       "top        Lamar MS\n",
       "freq            144\n",
       "Name: School, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean['School'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 0' ' Little Cypress-Mauriceville' ' West Hardin CCISD' ...\n",
      " ' G.W. Carver Academy' ' Tioga' ' Townview Center']\n"
     ]
    }
   ],
   "source": [
    "school_df = df_clean[df_clean['School'].str.contains('HS') == False]\n",
    "school_df = school_df[school_df['School'].str.contains('MS') == False]\n",
    "school_df = school_df[school_df['School'].str.contains('JH') == False]\n",
    "print(school_df['School'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60269\n"
     ]
    }
   ],
   "source": [
    "# Remove DNA, DQ\n",
    "for i in judging_columns:\n",
    "    df_clean = df_clean[df_clean[i] != 'DNA']\n",
    "    df_clean = df_clean[df_clean[i] != 'DQ']\n",
    "\n",
    "# Convert blanks to nans\n",
    "for i in judging_columns:\n",
    "    df_clean[i] = df_clean[i].replace(['', ' '], np.nan)\n",
    "\n",
    "print(len(df_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average together Stage Judge 1, 2, and 3 into a new column\n",
    "\n",
    "df_clean['Stage Average'] = (df_clean['Stage Judge 1'] + df_clean['Stage Judge 2'] + df_clean['Stage Judge 3']) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort df_clean by year\n",
    "df_clean.sort_values(by=['Year'], inplace=True)\n",
    "\n",
    "# drop Event column\n",
    "df_clean = df_clean.drop(columns=['Event'])\n",
    "\n",
    "# drop TEA column\n",
    "df_clean = df_clean.drop(columns=['TEA'])\n",
    "\n",
    "# drop ID column\n",
    "df_clean = df_clean.drop(columns=['ID'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all of the pml csv files into a dataframe\n",
    "files = glob.glob(\"csv_files/pml/*.csv\")\n",
    "pml_df = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
    "# convert pml_df to csv\n",
    "pml_df.to_csv(\"csv_files/pml_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['Date'] = df_clean['Date'].str.replace('DATE of EVENT ', '')\n",
    "judge_name_columns = ['cj1', 'cj2', 'cj3', 'srj1', 'srj2', 'srj3']\n",
    "for i in judge_name_columns:\n",
    "    df_clean[i] = df_clean[i].str.replace('1. ', '', regex=False)\n",
    "    df_clean[i] = df_clean[i].str.replace('2. ', '', regex=False)\n",
    "    df_clean[i] = df_clean[i].str.replace('3. ', '', regex=False)\n",
    "    # all lowercase\n",
    "    df_clean[i] = df_clean[i].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each judge name column, remove everything after the first comma\n",
    "for i in judge_name_columns:\n",
    "    #df_clean[i] = df_clean[i].str.split(',', expand=True)[0]\n",
    "    #df_clean[i] = df_clean[i].str.split('-', expand=True)[0]\n",
    "    # trim whitespace\n",
    "    df_clean[i] = df_clean[i].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = df_clean.columns\n",
    "# trim whitespace from all columns\n",
    "for i in all_columns:\n",
    "    try:\n",
    "        df_clean[i] = df_clean[i].str.strip()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where School = 0\n",
    "df_clean = df_clean[df_clean['School'] != '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rows where year is greater than 3000\n",
    "df_clean_yr_error = df_clean[df_clean['Year'] > 3000]\n",
    "\n",
    "# drop rows where year is greater than 3000\n",
    "df_clean = df_clean[df_clean['Year'] < 3000]\n",
    "\n",
    "df_clean_yr_error['Year'] = df_clean_yr_error['Classification']\n",
    "df_clean_yr_error['Classification'] = df_clean_yr_error['Conference']\n",
    "\n",
    "# fill conference column with blanks\n",
    "df_clean_yr_error['Conference'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine df_clean and df_clean_yr_error\n",
    "df_clean = pd.concat([df_clean, df_clean_yr_error], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>School</th>\n",
       "      <th>City</th>\n",
       "      <th>Directors</th>\n",
       "      <th>Conference</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Year</th>\n",
       "      <th>Stage Judge 1</th>\n",
       "      <th>Stage Judge 2</th>\n",
       "      <th>Stage Judge 3</th>\n",
       "      <th>Stage Final</th>\n",
       "      <th>...</th>\n",
       "      <th>cj1</th>\n",
       "      <th>cj2</th>\n",
       "      <th>cj3</th>\n",
       "      <th>srj1</th>\n",
       "      <th>srj2</th>\n",
       "      <th>srj3</th>\n",
       "      <th>Selection 1 Comp/arr</th>\n",
       "      <th>Selection 2 Comp/arr</th>\n",
       "      <th>Selection 3 Comp/arr</th>\n",
       "      <th>Stage Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>James Bowie HS</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>Larry Brown</td>\n",
       "      <td>AAAAA</td>\n",
       "      <td>Non-Varsity</td>\n",
       "      <td>2005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>richard bass</td>\n",
       "      <td>joe frank, jr.</td>\n",
       "      <td>rodney klett</td>\n",
       "      <td>george jones</td>\n",
       "      <td>tom neugent</td>\n",
       "      <td>marion west</td>\n",
       "      <td>Chambers</td>\n",
       "      <td>Grainger/Rogers/SMC</td>\n",
       "      <td>Gregson</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Munday HS</td>\n",
       "      <td>Munday</td>\n",
       "      <td>Rodney D. Bennett</td>\n",
       "      <td>A</td>\n",
       "      <td>Varsity</td>\n",
       "      <td>2005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>mike glaze</td>\n",
       "      <td>richard herrera</td>\n",
       "      <td>will burks</td>\n",
       "      <td>harold bufe</td>\n",
       "      <td>mack bibb</td>\n",
       "      <td>june bearden</td>\n",
       "      <td>King/Glover</td>\n",
       "      <td>Stuart</td>\n",
       "      <td>Swearingen</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Holliday HS</td>\n",
       "      <td>Holliday</td>\n",
       "      <td>Melanie Hadderton</td>\n",
       "      <td>AA</td>\n",
       "      <td>Varsity</td>\n",
       "      <td>2005</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>mike glaze</td>\n",
       "      <td>richard herrera</td>\n",
       "      <td>will burks</td>\n",
       "      <td>harold bufe</td>\n",
       "      <td>mack bibb</td>\n",
       "      <td>june bearden</td>\n",
       "      <td>Carl King</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Meyer</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decatur HS</td>\n",
       "      <td>Decatur</td>\n",
       "      <td>Doug Fulwood</td>\n",
       "      <td>AAA</td>\n",
       "      <td>Varsity</td>\n",
       "      <td>2005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>mike glaze</td>\n",
       "      <td>richard herrera</td>\n",
       "      <td>will burks</td>\n",
       "      <td>harold bufe</td>\n",
       "      <td>mack bibb</td>\n",
       "      <td>june bearden</td>\n",
       "      <td>King/Swearingen</td>\n",
       "      <td>David Black</td>\n",
       "      <td>Robert Sheldon</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>City View HS</td>\n",
       "      <td>Wichita Falls</td>\n",
       "      <td>Terah Kay Shawver</td>\n",
       "      <td>AA</td>\n",
       "      <td>Varsity</td>\n",
       "      <td>2005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>mike glaze</td>\n",
       "      <td>richard herrera</td>\n",
       "      <td>will burks</td>\n",
       "      <td>harold bufe</td>\n",
       "      <td>mack bibb</td>\n",
       "      <td>june bearden</td>\n",
       "      <td>LaPlante</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Strommen</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60225</th>\n",
       "      <td>Alvarado MS</td>\n",
       "      <td>Alvarado</td>\n",
       "      <td>Kelli Bahner / Joe Gunn</td>\n",
       "      <td></td>\n",
       "      <td>Non-Varsity</td>\n",
       "      <td>2014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>christine cumberledge</td>\n",
       "      <td>james marioneaux</td>\n",
       "      <td>julie amos</td>\n",
       "      <td>corey ash</td>\n",
       "      <td>harold bufe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Owens/</td>\n",
       "      <td>Murtha</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60226</th>\n",
       "      <td>Alvarado MS</td>\n",
       "      <td>Alvarado</td>\n",
       "      <td>Kelli Bahner / Joe Gunn</td>\n",
       "      <td></td>\n",
       "      <td>Varsity</td>\n",
       "      <td>2014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>christine cumberledge</td>\n",
       "      <td>james marioneaux</td>\n",
       "      <td>julie amos</td>\n",
       "      <td>corey ash</td>\n",
       "      <td>harold bufe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Himes/</td>\n",
       "      <td>Concert March</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60227</th>\n",
       "      <td>Del Rio HS</td>\n",
       "      <td>Del Rio</td>\n",
       "      <td>Daniel White</td>\n",
       "      <td></td>\n",
       "      <td>Sub Non-Varsity</td>\n",
       "      <td>2014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>bob whipkey</td>\n",
       "      <td>rogerio olivarez</td>\n",
       "      <td>charles cabrera</td>\n",
       "      <td>kyle friesenhahn</td>\n",
       "      <td>juan sosa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Standridge/</td>\n",
       "      <td>King/Swearingen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60228</th>\n",
       "      <td>Stinson MS</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>Kevin Leman / Alex Melendez</td>\n",
       "      <td></td>\n",
       "      <td>Non-Varsity</td>\n",
       "      <td>2014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>kim rosenberg</td>\n",
       "      <td>javier vera</td>\n",
       "      <td>james snider</td>\n",
       "      <td>cathy teltschik</td>\n",
       "      <td>larry wolf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gazlay/</td>\n",
       "      <td>Smith/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60229</th>\n",
       "      <td>Del Rio HS</td>\n",
       "      <td>Del Rio</td>\n",
       "      <td>Daniel White</td>\n",
       "      <td></td>\n",
       "      <td>Non-Varsity</td>\n",
       "      <td>2015</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>ronnie rios</td>\n",
       "      <td>james snider</td>\n",
       "      <td>alfonso alvarado</td>\n",
       "      <td>john faraone</td>\n",
       "      <td>roger olivarez</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sheldon/</td>\n",
       "      <td>Anon or Trad/Barker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60230 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               School           City                    Directors Conference  \\\n",
       "0      James Bowie HS      Arlington                  Larry Brown      AAAAA   \n",
       "1           Munday HS         Munday            Rodney D. Bennett          A   \n",
       "2         Holliday HS       Holliday            Melanie Hadderton         AA   \n",
       "3          Decatur HS        Decatur                 Doug Fulwood        AAA   \n",
       "4        City View HS  Wichita Falls            Terah Kay Shawver         AA   \n",
       "...               ...            ...                          ...        ...   \n",
       "60225     Alvarado MS       Alvarado      Kelli Bahner / Joe Gunn              \n",
       "60226     Alvarado MS       Alvarado      Kelli Bahner / Joe Gunn              \n",
       "60227      Del Rio HS        Del Rio                 Daniel White              \n",
       "60228      Stinson MS    San Antonio  Kevin Leman / Alex Melendez              \n",
       "60229      Del Rio HS        Del Rio                 Daniel White              \n",
       "\n",
       "        Classification  Year  Stage Judge 1  Stage Judge 2  Stage Judge 3  \\\n",
       "0          Non-Varsity  2005            1.0            1.0            1.0   \n",
       "1              Varsity  2005            1.0            1.0            2.0   \n",
       "2              Varsity  2005            2.0            2.0            3.0   \n",
       "3              Varsity  2005            1.0            2.0            2.0   \n",
       "4              Varsity  2005            1.0            1.0            1.0   \n",
       "...                ...   ...            ...            ...            ...   \n",
       "60225      Non-Varsity  2014            1.0            3.0            1.0   \n",
       "60226          Varsity  2014            1.0            1.0            1.0   \n",
       "60227  Sub Non-Varsity  2014            1.0            1.0            1.0   \n",
       "60228      Non-Varsity  2014            1.0            1.0            1.0   \n",
       "60229      Non-Varsity  2015            2.0            2.0            2.0   \n",
       "\n",
       "       Stage Final  ...                    cj1               cj2  \\\n",
       "0              1.0  ...           richard bass    joe frank, jr.   \n",
       "1              1.0  ...             mike glaze   richard herrera   \n",
       "2              2.0  ...             mike glaze   richard herrera   \n",
       "3              2.0  ...             mike glaze   richard herrera   \n",
       "4              1.0  ...             mike glaze   richard herrera   \n",
       "...            ...  ...                    ...               ...   \n",
       "60225          2.0  ...  christine cumberledge  james marioneaux   \n",
       "60226          1.0  ...  christine cumberledge  james marioneaux   \n",
       "60227          1.0  ...            bob whipkey  rogerio olivarez   \n",
       "60228          1.0  ...          kim rosenberg       javier vera   \n",
       "60229          1.0  ...            ronnie rios      james snider   \n",
       "\n",
       "                    cj3              srj1            srj2          srj3  \\\n",
       "0          rodney klett      george jones     tom neugent   marion west   \n",
       "1            will burks       harold bufe       mack bibb  june bearden   \n",
       "2            will burks       harold bufe       mack bibb  june bearden   \n",
       "3            will burks       harold bufe       mack bibb  june bearden   \n",
       "4            will burks       harold bufe       mack bibb  june bearden   \n",
       "...                 ...               ...             ...           ...   \n",
       "60225        julie amos         corey ash     harold bufe           NaN   \n",
       "60226        julie amos         corey ash     harold bufe           NaN   \n",
       "60227   charles cabrera  kyle friesenhahn       juan sosa           NaN   \n",
       "60228      james snider   cathy teltschik      larry wolf           NaN   \n",
       "60229  alfonso alvarado      john faraone  roger olivarez           NaN   \n",
       "\n",
       "      Selection 1 Comp/arr Selection 2 Comp/arr Selection 3 Comp/arr  \\\n",
       "0                 Chambers  Grainger/Rogers/SMC              Gregson   \n",
       "1              King/Glover               Stuart           Swearingen   \n",
       "2                Carl King              Sheldon                Meyer   \n",
       "3          King/Swearingen          David Black       Robert Sheldon   \n",
       "4                 LaPlante              Sheldon             Strommen   \n",
       "...                    ...                  ...                  ...   \n",
       "60225               Owens/               Murtha                  NaN   \n",
       "60226               Himes/        Concert March                  NaN   \n",
       "60227          Standridge/      King/Swearingen                  NaN   \n",
       "60228              Gazlay/               Smith/                  NaN   \n",
       "60229             Sheldon/  Anon or Trad/Barker                  NaN   \n",
       "\n",
       "      Stage Average  \n",
       "0          1.000000  \n",
       "1          1.333333  \n",
       "2          2.333333  \n",
       "3          1.666667  \n",
       "4          1.000000  \n",
       "...             ...  \n",
       "60225      1.666667  \n",
       "60226      1.000000  \n",
       "60227      1.000000  \n",
       "60228      1.000000  \n",
       "60229      2.000000  \n",
       "\n",
       "[60230 rows x 30 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each column, remove all text after ' - '\n",
    "for i in judge_name_columns:\n",
    "    df_clean[i] = df_clean[i].str.split(' - ', expand=True)[0]\n",
    "    # trim whitespace\n",
    "    df_clean[i] = df_clean[i].str.strip()\n",
    "\n",
    "df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print unique city names\n",
    "city_list = df_clean['City'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the cities csv file\n",
    "\n",
    "\n",
    "cities_df = pd.read_csv('csv_files/cities/cities.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
     ]
    }
   ],
   "source": [
    "df_cities = df_clean\n",
    "# use fuzzywuzzy to match city names\n",
    "for i in city_list:\n",
    "    match = process.extractOne(i, cities_df['City'])\n",
    "    # replace city name with matched city name\n",
    "    df_cities['City'] = df_cities['City'].replace(i, match[0])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['City']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in judge_name_columns:\n",
    "    df_clean[each] = df_clean[each].str.replace(', lubbock', '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-harlingen\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", san antonio\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", sharyland\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", austin\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", moore\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", mission\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", edinburg north hs, edinburg cisd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", edinburg north hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", edinburg cisd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", lake travis\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", robstown hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", robstown\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", h.e.b.\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", duncanville\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", concert\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", sr\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", roma isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", roma\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", retired\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", corpus christi\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", la feria hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", la feria\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", iii\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", orange grove\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", weslaco\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", chair\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"(chair)\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", director fine arts\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", director of fine arts\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", tomball isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", baytown\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", ret.\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", edinburg hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", edinburg\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", juarez-lincoln hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", la joya isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", la joya\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", alvarado\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", mcallen isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", mcallen\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", united isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", laredo\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", mcqueeny\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", seguin\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", mission cisd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", connally hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", el paso\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", el paso isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", brownsville isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", brownsville\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", inst.musicadv.\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", grulla hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", rio grande city isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", rio grande city\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", midway hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", woodway\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", tuloso-midway hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", ac blunt ms\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", aransas pass\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", leander\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", falfurrias hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", falfurrias\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", kingsville\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-retired\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", weslaco isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", fine arts administrator\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\",la joya isd\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", aledo\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", canyon\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", p.s.j.a.\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", odem hs\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", c.o. wilson ms\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", garland\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", houston\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-banda\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"*\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"--\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"1\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"2\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"3\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"4\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"5\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"6\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"xxx\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"virginia osolvsky\", 'virginia olsovsky', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"virginia osovsky\", 'virginia olsovsky', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"virginian olsovsky\", 'virginia olsovsky', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"villareal\", 'villarreal', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-flour bluff\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\" d. \", ' ', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"wallace diefolf\", 'wallace dierolf', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"walace dierolf\", 'wallace dierolf', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"unknown\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"ty ann payne\", 'tye ann payne', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"tye ann payne\", 'tye payne', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-bastrop\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"(10th)\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"(11th)\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"allmany\", 'almany', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"mcelory\", 'mcelroy', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"knolficek\", 'knoflicek', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"knloficek\", 'knoflicek', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"knofllicek\", 'knoflicek', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"tom herrington\", 'tom harrington', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\" (chrmn.)\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\" (chmn)\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\" (chmn.)\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"clearwarter\", 'clearwater', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\" .j \", ' ', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"tim edens\", 'tim edins', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"tim andersen\", 'tim anderson', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"terri brockway\", 'teri brockway', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"tbd/\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"tammy fedenych\", 'tammy fedynich', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"tammy fedinich\", 'tammy fedynich', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"tammy fednmich\", 'tammy fedynich', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"susan meyer-patterson\", 'susan patterson', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"susan meyer patterson\", 'susan patterson', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-houston\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-pearsall\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-psja\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-corpus christi\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-keller\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"steel, jason\", 'steele, jason', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"maudlin\", 'mauldin', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"stacy claek\", 'stacy clark', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"stacy clark\", 'stacey clark', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"sr #1\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"sr #2\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"sr #3\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"4/11/17\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"4/13/17\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"see region 21 web site\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"see original event\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"see original contest\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"see original evenrt\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"sheppard\", 'shepherd', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-beaumont\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"sandy bow brunskill\", 'sandra brunskill', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"sandy brunskill\", 'sandra brunskill', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"sandra bow brunskill\", 'sandra brunskill', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"sandra bow-brunskill\", 'sandra brunskill', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"rylon guidory\", 'rylon guidry', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-austin\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"ryan straten\", 'ryan stratten', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-mabank\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"rusty honeycutt\", 'rustin honeycutt', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"barerra\", 'barrera', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"costellano\", 'castellano', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\" m. \", ' ', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\",san antonio\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\", frisco\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"/poteet\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\" (8th)\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\" (9th)\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-spring\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"margarit\", 'margaret', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"nv-\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"/v-\", '/', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"//\", '/', regex=False)\n",
    "    #df_clean[each] = df_clean[each].str.replace(\"^x?\", '', regex=True)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"^tba?\", '', regex=True)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"^tbd?\", '', regex=True)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"- -\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"-plano\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\" .k \", ' ', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"bene davis \", 'ben davis', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"benny davis\", 'ben davis', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"see region website\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"see region web site\", '', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"rick yancy\", 'rick yancey', regex=False)\n",
    "    df_clean[each] = df_clean[each].str.replace(\"phillip alvarado\", 'phil alvarado', regex=False)\n",
    "    #df_clean[each] = df_clean[each].str.replace(\"^d?\", '', regex=True)\n",
    "    #df_clean[each] = df_clean[each].str.replace(\"^f?\", '', regex=True)\n",
    "    #df_clean[each] = df_clean[each].str.replace(\"^judge?\", '', regex=True)\n",
    "    #df_clean[each] = df_clean[each].str.replace(\"^b?\", '', regex=True)\n",
    "    #df_clean[each] = df_clean[each].str.replace(\"^c?\", '', regex=True)\n",
    "    #df_clean[each] = df_clean[each].str.replace(\"^a?\", '', regex=True)\n",
    "    #df_clean[each] = df_clean[each].str.replace(\"^e?\", '', regex=True)\n",
    "    #df_clean[each] = df_clean[each].str.replace(\"^t?\", '', regex=True)\n",
    "\n",
    "    df_clean[each] = df_clean[each].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to csv\n",
    "df_clean.to_csv(\"csv_files/full_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53babf5c189ad8a4933a2bd5063a1840a2c47c9b82a6c06a0d6cefa0137ac8c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
